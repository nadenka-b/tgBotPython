import asyncio
import re
import aiohttp
import logging
import pandas as pd
from bs4 import BeautifulSoup, Tag
from typing import Optional

logger = logging.getLogger(__name__)

CATEGORIES = {
    '–Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Å—Ç–∞': '–æ–±—â–∏–π –∫–æ–Ω–∫—É—Ä—Å',
    '—Ü–µ–ª–µ–≤–æ–π –∫–≤–æ—Ç—ã': '—Ü–µ–ª–µ–≤–∞—è –∫–≤–æ—Ç–∞',
    '–æ—Å–æ–±–æ–π –∫–≤–æ—Ç—ã': '–æ—Å–æ–±–∞—è –∫–≤–æ—Ç–∞',
    '–æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–≤–æ—Ç—ã': '–æ—Ç–¥–µ–ª—å–Ω–∞—è –∫–≤–æ—Ç–∞',
    '–±–µ–∑ –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—ã—Ö –∏—Å–ø—ã—Ç–∞–Ω–∏–π': '–±–µ–∑ –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—ã—Ö –∏—Å–ø—ã—Ç–∞–Ω–∏–π',
}

NEEDED_COLUMNS = {
    '–£–Ω–∏–∫–∞–ª—å–Ω—ã–π id –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–∞ –ï–ü–ì–£': 'epgu_id',
    'id –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–∞': 'applicant_id',
    '–°—É–º–º–∞ –∫–æ–Ω–∫—É—Ä—Å–Ω—ã—Ö –±–∞–ª–ª–æ–≤': 'score',
    '–ó–∞—è–≤–ª–µ–Ω–∏–µ –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ –∑–∞—á–∏—Å–ª–µ–Ω–∏–µ': 'agreement',
    '–°—Ç–∞—Ç—É—Å': 'status',
    '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ': 'note',
}

CATEGORY_MAPPING = {
    '–æ—Å–æ–±–æ–π –∫–≤–æ—Ç—ã': '–æ—Å–æ–±–∞—è –∫–≤–æ—Ç–∞',
    '—Ü–µ–ª–µ–≤–æ–π –∫–≤–æ—Ç—ã': '—Ü–µ–ª–µ–≤–∞—è –∫–≤–æ—Ç–∞',
    '–æ—Ç–¥–µ–ª—å–Ω–∞—è –∫–≤–æ—Ç–∞': '–æ—Ç–¥–µ–ª—å–Ω–∞—è –∫–≤–æ—Ç–∞'
}


class Parser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ –ö–§–£"""

    def __init__(self, session: aiohttp.ClientSession, base_url: str):
        self.session: Optional[aiohttp.ClientSession] = session
        self.base_url = base_url

    async def fetch_page(self, params: dict[str, str]) -> str:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        params: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è URL
        """
        try:
            if not self.session:
                logging.warning("–°–µ—Å—Å–∏—è –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
                return ""
            async with self.session.get(self.base_url, params=params) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    logger.error(
                        f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: —Å—Ç–∞—Ç—É—Å {response.status}")
                    return ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")
            return ""

    def extract_filter_options(self, html: str, filter_name: str) -> list[tuple]:
        """
        –ò–∑–≤–ª–µ—á—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–ø—Ü–∏–∏ –∏–∑ —Å–µ–ª–µ–∫—Ç–∞
        filter_name: 'level', 'faculty', 'inst', 'speciality', 'typeofstudy', 'category'
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (value, label)
        """
        soup = BeautifulSoup(html, 'lxml')

        select = soup.find('select', {'name': f"p_{filter_name}"})
        if not select:
            return []

        options = []
        for item in select.find_all("option"):
            value = item.get('value')
            text = item.get_text(strip=True)
            if not value or not text or "(–¥–ª—è –ø—Ä–∏–µ–º–∞ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö –≥—Ä–∞–∂–¥–∞–Ω)" in text:
                continue

            value = str(value).strip()

            if filter_name == 'speciality':
                text = text.split('(')[0].strip()

            options.append((value, text))

        return options

    def _extract_admission_category(self, table) -> str:
        """
        –ò–∑–≤–ª–µ—á—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø—Ä–∏–µ–º–∞ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø–µ—Ä–µ–¥ —Ç–∞–±–ª–∏—Ü–µ–π
        –ò—â–µ—Ç —Ç–µ–∫—Å—Ç —Ç–∏–ø–∞: "... –æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–≤–æ—Ç—ã" –∏ –±–µ—Ä–µ—Ç –∏–º–µ–Ω–Ω–æ "–æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–≤–æ—Ç—ã"

        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π (e.g., "–æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–≤–æ—Ç—ã", "—Ü–µ–ª–µ–≤–æ–π –∫–≤–æ—Ç—ã") –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        """
        overflow_div = table.find_parent('div', {'class': 'overflow-table'})

        if not overflow_div:
            logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω div.overflow-table")
            return ""

        previous = overflow_div.find_previous(
            ['h1', 'h2', 'h3', 'h4', 'p', 'div'])

        if not previous:
            logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–µ—Ä–µ–¥ —Ç–∞–±–ª–∏—Ü–µ–π")
            return ""

        text = previous.get_text(strip=True)
        logger.debug(f"üîç –ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ —Ç–∞–±–ª–∏—Ü–µ–π: {text}")

        for key, category_name in CATEGORIES.items():
            if key.lower() in text.lower():
                logger.debug(f"‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category_name}")
                return category_name

        return text[:50]

    def _extract_admission_plan(self, soup: BeautifulSoup) -> dict[str, int]:
        """
        –ò–∑–≤–ª–µ—á—å –ø–ª–∞–Ω –ø—Ä–∏–µ–º–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º

        Returns: {'–æ–±—â–∏–π –∫–æ–Ω–∫—É—Ä—Å': 71,
                '—Ü–µ–ª–µ–≤–∞—è –∫–≤–æ—Ç–∞': 8,...}
        """

        plan_div = soup.find('div', {'class': 'listing-abitur__plan'})

        if not plan_div:
            logger.warning("‚ö†Ô∏è –ë–ª–æ–∫ –ø–ª–∞–Ω–∞ –ø—Ä–∏–µ–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return {}

        plan_data = {}

        plan_strong = plan_div.find('p')
        if plan_strong:
            plan_text = plan_strong.get_text(strip=True)
            match = re.search(r'\d+', plan_text)
            if match:
                total_plan = int(match.group())
                plan_data['–æ–±—â–∏–π –∫–æ–Ω–∫—É—Ä—Å'] = total_plan
                logger.debug(f"üìä –û–±—â–∏–π –ø–ª–∞–Ω –ø—Ä–∏–µ–º–∞: {total_plan}")

        lis = plan_div.find_all('li')

        for li in lis:
            li_text = li.get_text(strip=True)
            logger.debug(f"üîç –°—Ç—Ä–æ–∫–∞ –ø–ª–∞–Ω–∞: {li_text}")

            for key, category_name in CATEGORY_MAPPING.items():
                if key.lower() in li_text.lower():
                    match = re.search(r'(\d+)(?:\s|$)', li_text)
                    if match:
                        seats = int(match.group(1))
                        plan_data[category_name] = seats
                        logger.debug(f"‚úÖ {category_name}: {seats} –º–µ—Å—Ç")
                    break

        logger.info(f"üìã –ü–ª–∞–Ω –ø—Ä–∏–µ–º–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {plan_data}")
        return plan_data

    def _extract_headers(self, table: Tag, table_idx: int, admission_category: str) -> dict[str, int] | None:
        """
        –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã –∏ –Ω–∞–π—Ç–∏ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã –≤ —Ç–∞–±–ª–∏—Ü–µ

        Returns: {'–£–Ω–∏–∫–∞–ª—å–Ω—ã–π id –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–∞ –ï–ü–ì–£': 1,
                'id –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–∞': 2,...}
        """
        headers = []
        thead = table.find('thead')
        if thead:
            header_row = thead.find("tr")
            if header_row:
                header_cells = header_row.find_all(
                    class_="tablebig__th")
                for cell in header_cells:
                    text = cell.get_text(strip=True)
                    if text:
                        headers.append(text)

        if not headers:
            logger.warning(
                f"‚ö†Ô∏è –¢–∞–±–ª–∏—Ü–∞ {table_idx + 1} –Ω–µ –∏–º–µ–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
            return None

        logger.debug(f"–ó–∞–≥–æ–ª–æ–≤–∫–∏: {headers}")

        column_indices = {}
        headers = list(map(str.lower, headers))

        for header_name, column_name in NEEDED_COLUMNS.items():
            if header_name.lower() in headers:
                idx = headers.index(header_name.lower())
                column_indices[column_name] = idx
                logger.debug(
                    f"‚úÖ –ù–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü '{column_name}' –ø–æ –∏–Ω–¥–µ–∫—Å—É {idx}")
            elif admission_category == '–±–µ–∑ –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—ã—Ö –∏—Å–ø—ã—Ç–∞–Ω–∏–π' and header_name == '–°—É–º–º–∞ –∫–æ–Ω–∫—É—Ä—Å–Ω—ã—Ö –±–∞–ª–ª–æ–≤':
                continue
            else:
                logger.warning(
                    f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü '{column_name}' –≤ —Ç–∞–±–ª–∏—Ü–µ {table_idx + 1}")
                return None

        return column_indices

    def extract_table_data(self, html: str) -> pd.DataFrame | None:
        """
        –ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü
        –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã —Å –∫–ª–∞—Å—Å–æ–º tablebig, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö
        """
        soup = BeautifulSoup(html, 'lxml')

        admission_plans = self._extract_admission_plan(soup)
        logger.debug(f"üìã –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –ø–ª–∞–Ω: {admission_plans}")

        tables = soup.find_all('table', {'class': 'tablebig'})
        if not tables:
            logger.warning("‚ö†Ô∏è –¢–∞–±–ª–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return None

        logger.debug(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")

        all_data = []

        for table_idx, table in enumerate(tables):
            logger.debug(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–∞–±–ª–∏—Ü—É {table_idx + 1}")
            try:
                admission_category = self._extract_admission_category(table)
                available_seats = admission_plans.get(admission_category, 0)
                logger.debug(f"üìå –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø—Ä–∏–µ–º–∞: {admission_category}")

                column_indices = self._extract_headers(
                    table, table_idx, admission_category)

                if not column_indices:
                    continue

                rows = table.find_all('tr')[1:]
                for _, tr in enumerate(rows):
                    tds = tr.find_all('td')

                    if not tds:
                        continue

                    row_data = {}
                    for key, col_idx in column_indices.items():
                        if col_idx < len(tds):
                            cell_text = tds[col_idx].get_text(strip=True)
                            row_data[key] = cell_text if cell_text else None
                        else:
                            row_data[key] = None

                    row_data['admission_category'] = admission_category
                    row_data['available_seats'] = available_seats
                    all_data.append(row_data)

                logger.debug(
                    f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ {table_idx + 1}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(all_data)} —Å—Ç—Ä–æ–∫")

            except Exception as e:
                logger.error(
                    f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–∞–±–ª–∏—Ü—ã {table_idx + 1}: {e}")
                continue

        if not all_data:
            logger.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return None

        df = pd.DataFrame(all_data)
        logger.info(
            f"‚úÖ –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {len(tables)} —Ç–∞–±–ª–∏—Ü")

        return df

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()
